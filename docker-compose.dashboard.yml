services:
  dashboard:
    image: ${DASHBOARD_IMAGE:-ghcr.io/obiente/cloud-dashboard:latest}
    # Build section removed - using image from GitHub Container Registry
    # To build locally, set DASHBOARD_IMAGE=obiente/cloud-dashboard:latest and uncomment build section below
    # build:
    #   context: .
    #   dockerfile: apps/dashboard/Dockerfile
    #   cache_from:
    #     - obiente/cloud-dashboard:latest
    environment:
      NODE_ENV: production
      PORT: 3000
      HOST: 0.0.0.0
      NUXT_SESSION_PASSWORD: ${SESSION_SECRET:-changeme}
      GITHUB_CLIENT_SECRET: ${GITHUB_CLIENT_SECRET:-}
      NUXT_PUBLIC_REQUEST_HOST: ${DASHBOARD_URL:-https://obiente.cloud}
      NUXT_PUBLIC_API_HOST: ${API_URL:-https://api.obiente.cloud}
      # Internal API host for server-side requests (uses Docker service name)
      # This allows the dashboard container to reach the API via internal networking
      NUXT_API_HOST_INTERNAL: ${API_HOST_INTERNAL:-http://api:3001}
      NUXT_PUBLIC_OIDC_ISSUER: ${ZITADEL_URL:-https://obiente.cloud}
      NUXT_PUBLIC_OIDC_BASE: ${ZITADEL_BASE_URL:-https://auth.obiente.cloud}
      NUXT_PUBLIC_OIDC_CLIENT_ID: ${ZITADEL_CLIENT_ID:-339499954043158530}
      NUXT_PUBLIC_GITHUB_CLIENT_ID: ${NUXT_PUBLIC_GITHUB_CLIENT_ID:-}
      NUXT_PUBLIC_DISABLE_AUTH: ${DISABLE_AUTH:-false}
      NUXT_PUBLIC_STRIPE_PUBLISHABLE_KEY: ${NUXT_PUBLIC_STRIPE_PUBLISHABLE_KEY:-}
      # Note: BILLING_ENABLED and SELF_HOSTED are fetched from API via GetPublicConfig endpoint
      # They are not needed here as the dashboard fetches them from the API
    networks:
      - obiente-network
    healthcheck:
      # Use node to make HTTP request (wget/curl not available in minimal alpine)
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3000/', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)}).on('error', () => process.exit(1))\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # Swarm deployment configuration for high availability
    deploy:
      # Configure replicas based on cluster size
      # Formula: replicas = ceil(cluster_size * DASHBOARD_REPLICAS_PERCENT / 100)
      # Example: 3 nodes * 50% = 1.5 → 2 replicas (minimum 2 for HA)
      # Set DASHBOARD_REPLICAS to override (e.g., DASHBOARD_REPLICAS=3)
      replicas: ${DASHBOARD_REPLICAS:-3}
      update_config:
        parallelism: 1  # Update one replica at a time to prevent mixed versions
        delay: 60s  # Wait longer between updates to ensure complete transition
        order: stop-first  # Stop old container before starting new (prevents load balancing to old versions)
        failure_action: rollback
        monitor: 90s  # Monitor for 90s to ensure new replica is fully healthy before continuing
      rollback_config:
        parallelism: 1
        delay: 10s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: "1"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M
      placement:
        # Spread across nodes - prefer manager nodes but allow workers if needed
        # Remove 'node.role == manager' constraint to allow deployment on worker nodes
        # This ensures multi-node deployment even with single manager node
        # Configure max replicas per node based on cluster size
        # Formula: max_per_node = ceil(replicas / cluster_size) or use percentage
        # Example: 5 replicas / 3 nodes = 1.67 → 2 per node
        # Set DASHBOARD_MAX_REPLICAS_PER_NODE to override (e.g., DASHBOARD_MAX_REPLICAS_PER_NODE=2)
        # For percentage-based: ceil(replicas * DASHBOARD_MAX_REPLICAS_PERCENT / 100 / cluster_size)
        max_replicas_per_node: ${DASHBOARD_MAX_REPLICAS_PER_NODE:-2}
        # Optional: Uncomment below to restrict to manager nodes only
        # constraints:
        #   - node.role == manager
      labels:
        # Traefik labels for automatic proxying (must be under deploy.labels for Swarm mode)
        # Dashboard uses the domain directly (no dashboard prefix)
        # Example: If DOMAIN=obiente.cloud, dashboard will be at https://obiente.cloud
        - "cloud.obiente.traefik=true"
        - "traefik.enable=true"
        # HTTP router (redirect to HTTPS)
        - "traefik.http.routers.dashboard.rule=Host(`${DOMAIN:-localhost}`)"
        - "traefik.http.routers.dashboard.entrypoints=web"
        - "traefik.http.routers.dashboard.service=dashboard"
        # HTTPS router (main)
        - "traefik.http.routers.dashboard-secure.rule=Host(`${DOMAIN:-localhost}`)"
        - "traefik.http.routers.dashboard-secure.entrypoints=websecure"
        - "traefik.http.routers.dashboard-secure.service=dashboard"
        - "traefik.http.routers.dashboard-secure.tls=true"
        - "traefik.http.routers.dashboard-secure.tls.certresolver=letsencrypt"
        # Service configuration with health checks to prevent routing to old replicas
        - "traefik.http.services.dashboard.loadbalancer.server.port=3000"
        - "traefik.http.services.dashboard.loadbalancer.healthcheck.path=/"
        - "traefik.http.services.dashboard.loadbalancer.healthcheck.interval=10s"
        - "traefik.http.services.dashboard.loadbalancer.healthcheck.timeout=5s"
        - "traefik.http.services.dashboard.loadbalancer.passhostheader=true"
        # Sticky sessions to ensure users stay on same replica
        - "traefik.http.services.dashboard.loadbalancer.sticky.cookie=true"
        - "traefik.http.services.dashboard.loadbalancer.sticky.cookie.name=dashboard_session"

networks:
  obiente-network:
    external: true
    name: obiente_obiente-network
    # Network is created by main stack (docker-compose.swarm.yml)
    # with subnet: 10.0.9.0/24
