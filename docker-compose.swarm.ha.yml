# Obiente Cloud - High Availability Docker Swarm Configuration
# This setup supports distributed deployments across multiple nodes

services:
  # ============================================================================
  # DISTRIBUTED DATABASE CLUSTER (PostgreSQL + Patroni)
  # ============================================================================

  # etcd - Distributed consensus for Patroni
  etcd:
    image: quay.io/coreos/etcd:v3.5.12
    command:
      - /usr/local/bin/etcd
      - --name=etcd
      - --initial-advertise-peer-urls=http://etcd:2380
      - --listen-peer-urls=http://0.0.0.0:2380
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://etcd:2379
      - --initial-cluster=etcd=http://etcd:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=obiente-cluster
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    volumes:
      - etcd_data:/etcd-data
    networks:
      - obiente-network

  # HAProxy - PostgreSQL connection pooler and load balancer
  pgpool:
    image: bitnami/pgpool:4
    environment:
      PGPOOL_BACKEND_NODES: 0:patroni-1:5432,1:patroni-2:5432,2:patroni-3:5432
      PGPOOL_SR_CHECK_USER: ${POSTGRES_USER:-obiente-postgres}
      PGPOOL_SR_CHECK_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      PGPOOL_ENABLE_LDAP: "no"
      PGPOOL_POSTGRES_USERNAME: ${POSTGRES_USER:-obiente-postgres}
      PGPOOL_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      PGPOOL_ADMIN_USERNAME: admin
      PGPOOL_ADMIN_PASSWORD: ${PGPOOL_ADMIN_PASSWORD:-adminpassword}
      PGPOOL_ENABLE_LOAD_BALANCING: "yes"
      PGPOOL_MAX_POOL: 100
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test: ["CMD", "/opt/bitnami/scripts/pgpool/healthcheck.sh"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - obiente-network

  # Patroni PostgreSQL - Node 1 (Leader)
  patroni-1:
    image: patroni/patroni:3.2.2
    hostname: patroni-1
    environment:
      PATRONI_NAME: patroni-1
      PATRONI_SCOPE: obiente-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: patroni-1:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: patroni-1:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${POSTGRES_USER:-obiente-postgres}
      PATRONI_SUPERUSER_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator-password}
      PATRONI_admin_PASSWORD: ${PATRONI_ADMIN_PASSWORD:-admin}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - patroni_1_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres.replica == 1
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "1"
          memory: 1G
    networks:
      - obiente-network

  # Patroni PostgreSQL - Node 2 (Replica)
  patroni-2:
    image: patroni/patroni:3.2.2
    hostname: patroni-2
    environment:
      PATRONI_NAME: patroni-2
      PATRONI_SCOPE: obiente-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: patroni-2:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: patroni-2:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${POSTGRES_USER:-obiente-postgres}
      PATRONI_SUPERUSER_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator-password}
      PATRONI_admin_PASSWORD: ${PATRONI_ADMIN_PASSWORD:-admin}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - patroni_2_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres.replica == 2
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "1"
          memory: 1G
    networks:
      - obiente-network

  # Patroni PostgreSQL - Node 3 (Replica)
  patroni-3:
    image: patroni/patroni:3.2.2
    hostname: patroni-3
    environment:
      PATRONI_NAME: patroni-3
      PATRONI_SCOPE: obiente-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: patroni-3:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: patroni-3:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${POSTGRES_USER:-obiente-postgres}
      PATRONI_SUPERUSER_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator-password}
      PATRONI_admin_PASSWORD: ${PATRONI_ADMIN_PASSWORD:-admin}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - patroni_3_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres.replica == 3
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "1"
          memory: 1G
    networks:
      - obiente-network

  # ============================================================================
  # METRICS DATABASE CLUSTER (TimescaleDB + Patroni)
  # ============================================================================

  # PgPool for TimescaleDB - Connection pooler and load balancer
  metrics-pgpool:
    image: bitnami/pgpool:4
    environment:
      PGPOOL_BACKEND_NODES: 0:metrics-patroni-1:5432,1:metrics-patroni-2:5432,2:metrics-patroni-3:5432
      PGPOOL_SR_CHECK_USER: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      PGPOOL_SR_CHECK_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      PGPOOL_ENABLE_LDAP: "no"
      PGPOOL_POSTGRES_USERNAME: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      PGPOOL_POSTGRES_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      PGPOOL_ADMIN_USERNAME: admin
      PGPOOL_ADMIN_PASSWORD: ${METRICS_PGPOOL_ADMIN_PASSWORD:-${PGPOOL_ADMIN_PASSWORD:-adminpassword}}
      PGPOOL_ENABLE_LOAD_BALANCING: "yes"
      PGPOOL_MAX_POOL: 100
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test: ["CMD", "/opt/bitnami/scripts/pgpool/healthcheck.sh"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - obiente-network

  # Patroni TimescaleDB - Node 1 (Leader)
  metrics-patroni-1:
    image: timescale/timescaledb:latest-pg16
    hostname: metrics-patroni-1
    environment:
      PATRONI_NAME: metrics-patroni-1
      PATRONI_SCOPE: metrics-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: metrics-patroni-1:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: metrics-patroni-1:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      PATRONI_SUPERUSER_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      PATRONI_REPLICATION_USERNAME: metrics-replicator
      PATRONI_REPLICATION_PASSWORD: ${METRICS_REPLICATION_PASSWORD:-${REPLICATION_PASSWORD:-metrics-replicator-password}}
      PATRONI_admin_PASSWORD: ${METRICS_PATRONI_ADMIN_PASSWORD:-${PATRONI_ADMIN_PASSWORD:-admin}}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - metrics_patroni_1_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.metrics.replica == 1
      resources:
        limits:
          cpus: "2"
          memory: 4G
        reservations:
          cpus: "1"
          memory: 2G
    networks:
      - obiente-network

  # Patroni TimescaleDB - Node 2 (Replica)
  metrics-patroni-2:
    image: timescale/timescaledb:latest-pg16
    hostname: metrics-patroni-2
    environment:
      PATRONI_NAME: metrics-patroni-2
      PATRONI_SCOPE: metrics-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: metrics-patroni-2:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: metrics-patroni-2:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      PATRONI_SUPERUSER_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      PATRONI_REPLICATION_USERNAME: metrics-replicator
      PATRONI_REPLICATION_PASSWORD: ${METRICS_REPLICATION_PASSWORD:-${REPLICATION_PASSWORD:-metrics-replicator-password}}
      PATRONI_admin_PASSWORD: ${METRICS_PATRONI_ADMIN_PASSWORD:-${PATRONI_ADMIN_PASSWORD:-admin}}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - metrics_patroni_2_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.metrics.replica == 2
      resources:
        limits:
          cpus: "2"
          memory: 4G
        reservations:
          cpus: "1"
          memory: 2G
    networks:
      - obiente-network

  # Patroni TimescaleDB - Node 3 (Replica)
  metrics-patroni-3:
    image: timescale/timescaledb:latest-pg16
    hostname: metrics-patroni-3
    environment:
      PATRONI_NAME: metrics-patroni-3
      PATRONI_SCOPE: metrics-cluster
      PATRONI_ETCD3_HOSTS: etcd:2379
      PATRONI_RESTAPI_CONNECT_ADDRESS: metrics-patroni-3:8008
      PATRONI_RESTAPI_LISTEN: 0.0.0.0:8008
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: metrics-patroni-3:5432
      PATRONI_POSTGRESQL_LISTEN: 0.0.0.0:5432
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_POSTGRESQL_PGPASS: /tmp/pgpass
      PATRONI_SUPERUSER_USERNAME: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      PATRONI_SUPERUSER_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      PATRONI_REPLICATION_USERNAME: metrics-replicator
      PATRONI_REPLICATION_PASSWORD: ${METRICS_REPLICATION_PASSWORD:-${REPLICATION_PASSWORD:-metrics-replicator-password}}
      PATRONI_admin_PASSWORD: ${METRICS_PATRONI_ADMIN_PASSWORD:-${PATRONI_ADMIN_PASSWORD:-admin}}
      PATRONI_admin_OPTIONS: "createdb,createrole"
    volumes:
      - metrics_patroni_3_data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.metrics.replica == 3
      resources:
        limits:
          cpus: "2"
          memory: 4G
        reservations:
          cpus: "1"
          memory: 2G
    networks:
      - obiente-network

  # ============================================================================
  # REDIS CLUSTER (for caching, sessions, job queues)
  # ============================================================================

  redis-1:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --port 6379
    volumes:
      - redis_1_data:/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.redis == 1
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    networks:
      - obiente-network

  redis-2:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --port 6379
    volumes:
      - redis_2_data:/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.redis == 2
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    networks:
      - obiente-network

  redis-3:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --port 6379
    volumes:
      - redis_3_data:/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.redis == 3
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    networks:
      - obiente-network

  # ============================================================================
  # OBIENTE CLOUD API SERVICES
  # ============================================================================

  # Go API - ConnectRPC service for deployments
  api:
    image: ${API_IMAGE:-ghcr.io/obiente/cloud-api:latest}
    # Build section removed - using image from GitHub Container Registry
    # To build locally, set API_IMAGE=obiente/cloud-api:latest and uncomment build section below
    # build:
    #   context: .
    #   dockerfile: apps/api/Dockerfile
    environment:
      PORT: ${GO_API_PORT:-3001}
      # Connect to PostgreSQL via PgPool
      DB_HOST: pgpool
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-obiente-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      DB_NAME: ${POSTGRES_DB:-obiente}
      # Metrics database (TimescaleDB via PgPool)
      METRICS_DB_HOST: metrics-pgpool
      METRICS_DB_PORT: 5432
      METRICS_DB_USER: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      METRICS_DB_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      METRICS_DB_NAME: ${METRICS_DB_NAME:-obiente_metrics}
      METRICS_COLLECTION_INTERVAL: ${METRICS_COLLECTION_INTERVAL:-5s}
      METRICS_STORAGE_INTERVAL: ${METRICS_STORAGE_INTERVAL:-60s}
      METRICS_LIVE_RETENTION: ${METRICS_LIVE_RETENTION:-5m}
      METRICS_MAX_WORKERS: ${METRICS_MAX_WORKERS:-50}
      METRICS_BATCH_SIZE: ${METRICS_BATCH_SIZE:-100}
      METRICS_MAX_LIVE_PER_DEPLOYMENT: ${METRICS_MAX_LIVE_PER_DEPLOYMENT:-1000}
      METRICS_MAX_PREVIOUS_STATS: ${METRICS_MAX_PREVIOUS_STATS:-10000}
      METRICS_DOCKER_API_TIMEOUT: ${METRICS_DOCKER_API_TIMEOUT:-10s}
      METRICS_DOCKER_API_RETRY_MAX: ${METRICS_DOCKER_API_RETRY_MAX:-3}
      METRICS_DOCKER_API_RETRY_BACKOFF_INITIAL: ${METRICS_DOCKER_API_RETRY_BACKOFF_INITIAL:-1s}
      METRICS_DOCKER_API_RETRY_BACKOFF_MAX: ${METRICS_DOCKER_API_RETRY_BACKOFF_MAX:-30s}
      METRICS_CIRCUIT_BREAKER_FAILURE_THRESHOLD: ${METRICS_CIRCUIT_BREAKER_FAILURE_THRESHOLD:-5}
      METRICS_CIRCUIT_BREAKER_COOLDOWN: ${METRICS_CIRCUIT_BREAKER_COOLDOWN:-1m}
      METRICS_CIRCUIT_BREAKER_HALFOPEN_MAX: ${METRICS_CIRCUIT_BREAKER_HALFOPEN_MAX:-3}
      METRICS_HEALTH_CHECK_INTERVAL: ${METRICS_HEALTH_CHECK_INTERVAL:-30s}
      METRICS_HEALTH_CHECK_FAILURE_THRESHOLD: ${METRICS_HEALTH_CHECK_FAILURE_THRESHOLD:-3}
      METRICS_SUBSCRIBER_BUFFER_SIZE: ${METRICS_SUBSCRIBER_BUFFER_SIZE:-100}
      METRICS_SUBSCRIBER_SLOW_THRESHOLD: ${METRICS_SUBSCRIBER_SLOW_THRESHOLD:-5s}
      METRICS_SUBSCRIBER_CLEANUP_INTERVAL: ${METRICS_SUBSCRIBER_CLEANUP_INTERVAL:-1m}
      METRICS_RETRY_MAX_RETRIES: ${METRICS_RETRY_MAX_RETRIES:-5}
      METRICS_RETRY_INTERVAL: ${METRICS_RETRY_INTERVAL:-1m}
      METRICS_RETRY_MAX_QUEUE_SIZE: ${METRICS_RETRY_MAX_QUEUE_SIZE:-10000}
      # Redis cluster endpoints
      REDIS_CLUSTER_NODES: redis-1:6379,redis-2:6379,redis-3:6379
      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-debug}
      # CORS Configuration
      CORS_ORIGIN: ${CORS_ORIGIN:-*}
      # Authentication - Zitadel
      ZITADEL_URL: ${ZITADEL_URL:-https://auth.obiente.cloud}
      ZITADEL_CLIENT_ID: ${ZITADEL_CLIENT_ID:-}
      ZITADEL_MANAGEMENT_TOKEN: ${ZITADEL_MANAGEMENT_TOKEN:-}
      ZITADEL_ORGANIZATION_ID: ${ZITADEL_ORGANIZATION_ID:-}
      DISABLE_AUTH: ${DISABLE_AUTH:-false}
      SKIP_TLS_VERIFY: ${SKIP_TLS_VERIFY:-false}
      SUPERADMIN_EMAILS: ${SUPERADMIN_EMAILS:-}
      # Docker Swarm API
      DOCKER_HOST: unix:///var/run/docker.sock
      # Deployment tracking
      NODE_ID: ${NODE_ID}
      SWARM_MANAGER_URL: ${SWARM_MANAGER_URL}
      # Pricing Configuration (optional - defaults are used if not set)
      PRICING_CPU_COST_PER_CORE_SECOND: ${PRICING_CPU_COST_PER_CORE_SECOND:-0.000000761}
      PRICING_MEMORY_COST_PER_BYTE_SECOND: ${PRICING_MEMORY_COST_PER_BYTE_SECOND:-0.000000000000001063}
      PRICING_BANDWIDTH_COST_PER_BYTE: ${PRICING_BANDWIDTH_COST_PER_BYTE:-0.000000000009313}
      PRICING_STORAGE_COST_PER_BYTE_MONTH: ${PRICING_STORAGE_COST_PER_BYTE_MONTH:-0.000000000186264}
      # Stripe Payment Processing
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY:-}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET:-}
      # Console/Dashboard URL for redirects (used by billing service)
      CONSOLE_URL: ${CONSOLE_URL:-${DASHBOARD_URL:-${APP_CONSOLE_URL:-https://app.obiente.cloud}}}
      DASHBOARD_URL: ${DASHBOARD_URL:-${CONSOLE_URL:-${APP_CONSOLE_URL:-https://app.obiente.cloud}}}
      APP_CONSOLE_URL: ${APP_CONSOLE_URL:-${CONSOLE_URL:-${DASHBOARD_URL:-https://app.obiente.cloud}}}
      # Support email (optional)
      SUPPORT_EMAIL: ${SUPPORT_EMAIL:-}
      # Swarm configuration - enable swarm features for HA swarm deployments
      ENABLE_SWARM: ${ENABLE_SWARM:-true} # Set to 'true' to enable Docker Swarm features (default: true for HA swarm)
    volumes:
      # Mount Docker socket for managing deployments
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount Docker volumes directory for named volumes (read-only)
      - /var/lib/docker/volumes:/var/lib/docker/volumes:ro
      # Mount custom safe directories for sanitized bind mounts
      - /var/lib/obiente:/var/lib/obiente
      - /tmp/obiente-volumes:/tmp/obiente-volumes
      - /tmp/obiente-deployments:/tmp/obiente-deployments
    depends_on:
      - pgpool
      - metrics-pgpool
      - redis-1
    deploy:
      mode: global # One instance per node
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: "2"
          memory: 1G
        reservations:
          cpus: "1"
          memory: 512M
      labels:
        - "cloud.obiente.service=api"
        - "cloud.obiente.role=control-plane"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:${GO_API_PORT:-3001}/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - obiente-network

  # ============================================================================
  # DEPLOYMENT ORCHESTRATOR
  # ============================================================================

  # Deployment orchestrator - manages user deployments across nodes
  deployment-orchestrator:
    image: obiente/deployment-orchestrator:latest
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-obiente-postgres}:${POSTGRES_PASSWORD:-obiente-postgres}@pgpool:5432/${POSTGRES_DB:-obiente}
      REDIS_CLUSTER_NODES: redis-1:6379,redis-2:6379,redis-3:6379
      DOCKER_HOST: unix:///var/run/docker.sock
      # Scheduling strategy
      DEPLOYMENT_STRATEGY: ${DEPLOYMENT_STRATEGY:-least-loaded}
      MAX_DEPLOYMENTS_PER_NODE: ${MAX_DEPLOYMENTS_PER_NODE:-50}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - pgpool
      - redis-1
      - api
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        order: start-first
      resources:
        limits:
          cpus: "1"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M
      labels:
        - "cloud.obiente.service=orchestrator"
        - "cloud.obiente.role=control-plane"
    networks:
      - obiente-network

  # ============================================================================
  # SERVICE DISCOVERY & ROUTING
  # ============================================================================

  # Traefik - Dynamic reverse proxy with service discovery
  traefik:
    image: traefik:v3.0
    command:
      - --api.dashboard=true
      - --api.insecure=false  # Use secure API (HTTPS only)
      - --providers.swarm=true  # Use Swarm provider for Docker Swarm mode (v3.0)
      - --providers.swarm.exposedbydefault=false
      - --providers.swarm.network=obiente-network
      - --providers.swarm.watch=true  # Watch for service changes
      - --providers.swarm.constraints=Label(`cloud.obiente.traefik`,`true`)  # Only discover services with this label
      - --providers.etcd.endpoints=etcd:2379
      - --providers.etcd.rootkey=traefik
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --entrypoints.deployments.address=:8000
      - --certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}
      - --certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json
      - --certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web
      - --metrics.prometheus=true
      - --accesslog=true
      - --log.level=INFO
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: ingress  # Ingress mode for Swarm load balancing
      - target: 443
        published: 443
        protocol: tcp
        mode: ingress  # Ingress mode for Swarm load balancing
      - target: 8000
        published: 8000
        protocol: tcp
        mode: ingress  # User deployments
      - target: 8080
        published: 8080
        protocol: tcp
        mode: ingress  # Traefik dashboard accessible on port 8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik_letsencrypt:/letsencrypt
    depends_on:
      - etcd
    deploy:
      mode: global  # Run on all nodes for HA and DNS to resolve properly
      update_config:
        parallelism: 1
        order: start-first
      placement:
        constraints:
          - node.labels.traefik.exclude != true  # Exclude nodes labeled with traefik.exclude=true
      resources:
        limits:
          cpus: "1"
          memory: 256M
        reservations:
          cpus: "0.5"
          memory: 128M
      labels:
        - "cloud.obiente.traefik=true"
        - "traefik.enable=true"
        - "traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)"
        - "traefik.http.routers.traefik.entrypoints=websecure"
        - "traefik.http.routers.traefik.tls.certresolver=letsencrypt"
        - "traefik.http.routers.traefik.service=api@internal"
        - "traefik.http.services.traefik.loadbalancer.server.port=8080"
        # API routing
        - "traefik.http.routers.api.rule=Host(`api.${DOMAIN}`)"
        - "traefik.http.routers.api.entrypoints=websecure"
        - "traefik.http.routers.api.tls.certresolver=letsencrypt"
        - "traefik.http.services.api.loadbalancer.server.port=3001"
    networks:
      - obiente-network

  # ============================================================================
  # DNS SERVER
  # ============================================================================

  # DNS Server - Resolves *.my.obiente.cloud domains
  # Configured to run on specific nodes using placement constraints
  dns:
    image: ${API_IMAGE:-ghcr.io/obiente/cloud-api:latest}
    # Build section removed - using image from GitHub Container Registry
    # To build locally, set API_IMAGE=obiente/cloud-api:latest and uncomment build section below
    # build:
    #   context: .
    #   dockerfile: apps/api/Dockerfile
    command: ["./dns-server"]
    environment:
      DB_HOST: pgpool
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-obiente-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      DB_NAME: ${POSTGRES_DB:-obiente}
      TRAEFIK_IPS: ${TRAEFIK_IPS:-} # Format: "region1:ip1,ip2;region2:ip3,ip4"
      DNS_IPS: ${DNS_IPS:-} # Comma-separated list of DNS server IPs
      DNS_PORT: ${DNS_PORT:-53} # DNS server port (default: 53)
    ports:
      - "${DNS_PORT:-53}:${DNS_PORT:-53}/udp"
      - "${DNS_PORT:-53}:${DNS_PORT:-53}/tcp"
    deploy:
      # Use placement constraints to control which nodes run DNS
      # By default, DNS runs on all nodes (global mode)
      # To delegate DNS to specific nodes:
      #   1. Set DNS_MODE=replicated in your .env file
      #   2. Set DNS_REPLICAS to the number of DNS instances you want
      #   3. Add node labels: docker node update --label-add dns.enabled=true <node-name>
      #   4. Uncomment/modify the constraint below to restrict DNS to specific nodes
      # Examples:
      #   - node.labels.dns.enabled == true (only runs on nodes with dns.enabled=true)
      #   - node.role == manager (only runs on manager nodes)
      #   - node.labels.dns.enabled != false (runs on all nodes except those with dns.enabled=false)
      # To disable DNS on specific nodes, label them: docker node update --label-add dns.enabled=false <node-name>
      mode: ${DNS_MODE:-global} # Options: global (all nodes), replicated (specific nodes)
      # replicas is only valid when mode is replicated
      # When using replicated mode, set DNS_REPLICAS environment variable and uncomment the line below
      # replicas: ${DNS_REPLICAS:-1} # Only used if mode is replicated
      placement:
        constraints:
          # Uncomment and modify the constraint below to restrict DNS to specific nodes
          # Example: Run only on nodes with dns.enabled=true label
          # - node.labels.dns.enabled == true
          # Example: Run only on manager nodes
          # - node.role == manager
          # Example: Run on all nodes except those with dns.enabled=false
          # - node.labels.dns.enabled != false
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    cap_add:
      - NET_BIND_SERVICE
    depends_on:
      - pgpool
    healthcheck:
      test: ["CMD-SHELL", "nslookup -type=A deploy-test.my.obiente.cloud localhost || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - obiente-network

  # ============================================================================
  # MONITORING & OBSERVABILITY
  # ============================================================================

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    user: "0:0"  # Run as root to access Docker socket for Swarm service discovery
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    volumes:
      - prometheus_data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Required for Docker Swarm service discovery
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M
      labels:
        - "cloud.obiente.traefik=true"
        - "traefik.enable=true"
        - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN}`)"
        - "traefik.http.routers.prometheus.entrypoints=websecure"
        - "traefik.http.routers.prometheus.tls.certresolver=letsencrypt"
        - "traefik.http.services.prometheus.loadbalancer.server.port=9090"
    networks:
      - obiente-network

  # Grafana - Visualization
  grafana:
    image: grafana/grafana:latest
    user: "0:0"  # Run as root for entrypoint script to write provisioning files
    entrypoint: ["/bin/sh", "/etc/grafana/entrypoint.sh"]
    command: ["/run.sh"]
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}
      # Disable strict provisioning validation to avoid conflicts in HA setup
      GF_PROVISIONING_DATASOURCES_ALLOW_DELETION: "true"
      # Pass database credentials as environment variables for datasource provisioning
      POSTGRES_USER: ${POSTGRES_USER:-obiente-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-obiente-postgres}
      METRICS_DB_USER: ${METRICS_DB_USER:-${POSTGRES_USER:-obiente-postgres}}
      METRICS_DB_PASSWORD: ${METRICS_DB_PASSWORD:-${POSTGRES_PASSWORD:-obiente-postgres}}
      # Datasource host configuration (for HA, use pgpool service names)
      GRAFANA_POSTGRES_HOST: ${GRAFANA_POSTGRES_HOST:-pgpool}
      GRAFANA_METRICS_DB_HOST: ${GRAFANA_METRICS_DB_HOST:-metrics-pgpool}
      # Alerting configuration
      ALERT_EMAIL: ${ALERT_EMAIL:-admin@example.com}
      # SMTP configuration for Grafana email notifications (only set if SMTP_HOST is configured)
      # These will be conditionally set by the entrypoint script
      SMTP_HOST: ${SMTP_HOST:-}
      SMTP_PORT: ${SMTP_PORT:-587}
      SMTP_USERNAME: ${SMTP_USERNAME:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      SMTP_FROM_ADDRESS: ${SMTP_FROM_ADDRESS:-}
      SMTP_FROM_NAME: ${SMTP_FROM_NAME:-Grafana}
      SMTP_SKIP_TLS_VERIFY: ${SMTP_SKIP_TLS_VERIFY:-false}
      SMTP_USE_STARTTLS: ${SMTP_USE_STARTTLS:-true}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:rw
      - ./monitoring/grafana/dashboards:/etc/grafana/dashboards:ro
      - ./monitoring/grafana/entrypoint.sh:/etc/grafana/entrypoint.sh:ro
    depends_on:
      - prometheus
      - pgpool
      - metrics-pgpool
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
      labels:
        - "cloud.obiente.traefik=true"
        - "traefik.enable=true"
        - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)"
        - "traefik.http.routers.grafana.entrypoints=web"
        - "traefik.http.routers.grafana.service=grafana"
        - "traefik.http.services.grafana.loadbalancer.server.port=3000"
        # Route Grafana on HTTPS for production access
        - "traefik.http.routers.grafana-secure.rule=Host(`grafana.${DOMAIN}`)"
        - "traefik.http.routers.grafana-secure.entrypoints=websecure"
        - "traefik.http.routers.grafana-secure.tls.certresolver=letsencrypt"
        - "traefik.http.routers.grafana-secure.service=grafana"
    networks:
      - obiente-network

# ============================================================================
# VOLUMES
# ============================================================================

volumes:
  # etcd
  etcd_data:
    driver: local

  # PostgreSQL Patroni
  patroni_1_data:
    driver: local
  patroni_2_data:
    driver: local
  patroni_3_data:
    driver: local

  # TimescaleDB Patroni
  metrics_patroni_1_data:
    driver: local
  metrics_patroni_2_data:
    driver: local
  metrics_patroni_3_data:
    driver: local

  # Redis Cluster
  redis_1_data:
    driver: local
  redis_2_data:
    driver: local
  redis_3_data:
    driver: local

  # Traefik
  traefik_letsencrypt:
    driver: local

  # Monitoring
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================

networks:
  obiente-network:
    external: true
    name: obiente_obiente-network
